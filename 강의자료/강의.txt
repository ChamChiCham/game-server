cpu

캐시 동기화

non uniform memory access
-> Multi-Core

uniform memory access
-> Multi-CPU

long -> int64_t

Multi-CPU 1. SMP 2. master/slave(거의x)
Multi-core

int 대신 short나 char 사용 -> 캐시 사용으로 메모리 절약
boolean -> bitwise 사용 boolean은 메모리가 큼.

캐시미스 -> 파이프라인이 논다.

--
memory
서버용:
Error 수정 기능이 있는 메모리 사용
-> 방사능에 의해 필연적으로 발생

27p 개인적으로 중요



--- 3/12 2.1 ---

5p recv는 scanf와 비슷.
1. 입력이 없으면 정지
2. 버퍼를 전부 채우지 않음.
그것이 fileio와 차이

6p
중요한데 시험엔 안나옴
넷기초에 나올수도?

7p
서버에 연결할 때 많은 라우터를 거쳐감
딜레이는 피할 수 없음.
택배랑 비슷

네트워킹이 허용하는 용량보다 덜 보내야 한다. (오버헤드 발생)

항상 최단거리로 가지는 않음. 먼저 출발한게 무조건 먼저 도착하지는 않음.

8p
네모박스를 코딩할 필요는 x 최적화하려면 어떻게 호출해야하는지만 알자

9p
패킷 구분 주의.
인터넷에서의 패킷과 application에서의 패킷은 다르다

a패킷은 패킷이 막 와도 내가 데이터를 구분할 수 있어야 한다.

12p
서버와 클라 모두 같은 프로토콜로 대화해야 한다.
언어와 비슷. 포맷이 단어면 순서는 문법

수정 -> 버전 넘버 변경
맨처음 서버와 클라이언트를 연결할 때 버전 먼저 확인
클라가 예전 클라일 수 있음.

14는 보고 넘어가자

15p
c에서의 struct는 가장 큰 자료형의 사이즈에 맞춘다. 이를 #pragma pack push, pop으로 빈칸없이 보낸다.
 
16p
보낼 때 a패킷이 쪼개질 수 있다.
하지만 네트워크가 끊기지 않는다면 도착하는 a패킷의 총 크기가 달라지지는 않는다.
 => 받는 쪽에서 쪼개진 a패킷 재조립이 필요하다. => 알아서 해줌.

--3/15 2.2 --


h to ns(): setting little endian, big endian
// 포인터를 네트워크로
p to n()

Recv
MSG_PEEK: OS queue에서 보기만 함
MSG_OOB: 큐에 쌓인 데이터를 무시하고 받음

IpNumberofBytesRecvd: 0이면 오류


accept에서 
IpfnCondition을 사용해서 거절하는 것이 오버헤드가 적다.

그래도 오버헤드를 막을 순 없다. 별도 하드웨어를 사용해서 차단하는 것이 효율적


== 3/19 3.1 ==

Non-blocking I/O


Socket Thread
-> Coroutine API c++에서는 지금은 사용 불가.
시도한다면 GO나 Erlang

Select
개수의 한계를 늘릴순 있지만 오버헤드가 개수에 비례하므로 성능이 떨어진다.

WSAAsyncSelect
소켓 이벤트를 윈도우의 메시지로 받음 => 서버엔 사용하지 않음.

WSAWaitForMultipleEvents()
event의 array=> 배열 안의 이벤트가 발생했을 경우 socket 추출
기다릴수도 있고 안기다릴수도 있다. -> bWaitAll
소캣 64개의 제한이 있기 때문에 사용하지 않음.
대규모 서버에서는 사용하지 않을 예정.

--여기까지가 고전적인 모델이다. 이후론 현대적인 모델

우리는 Overlapped I/O 를 사용할 것이다.
클라는 상관없는데 서버는 무조건 Overlapped I/O 사용해야함.

send() recv()가 독립적이 아니라 겹친다.
IOCP는 Overlapped I/O 위에 돌아간다.
Overlapped I/O 먼저 이해하면 IOCP 할만하다.

요청 후 즉시 리턴. 실패를 하는 것이 아니라 결과가 뒤에 온다.
한번 recv를 하면 거의 무조건 온다.
데이터가 도착한 순서대로 처리를 한다.

overlapped i/0에서 recv queue는 일종의 hash table이다.

소켓을 만들지 않았는데 정보를 보낸다?
커널 메모리에 쌓아 둔다. 
이것도 싫다 하면 쌓아두지 않게 만들 수 있다.
=> 실패했다고 전달. 클라이언트가 재전송하게 한다.

지금은 사용 x
요즘엔 그냥 커널 메모리에 잠깐 쌓게 한다. => 메모리 크기의 증가.


recv할떄 이미 도착한 데이터가 있으면 받을수도 있지만 굳이? 프로그래밍이 지저분해짐.
애초에 그럴 확률이 매우 적음.

하나의 소켓에 두개 이상의 recv를 받게 되면 보낸 순서대로 처리가 안될 수 있음.


=== 3/22 3.2===

completionROUTINE은 모든 소켓이 공통으로 사용한다
-> 어떤 소켓에 대한 send/recv인가? send인가 recv인가? 같은 정보를 알아내야 함.

43p
누가?: 운영체제 / 언제?:
signal handler는 까다로워 사용하지 않음.

systemcall을 주기적으로 해야 callback이 정상 작동??.

NON over이든 over이든 상관 x 데이터 주고받을 수 있음.

55p 아래설명 필요 x

=== 3/26 4.1 ===
4-65p 추가
패킷에 들어가는 데이터의 크기(char, short, int, long 등)을 같은 형태로 유지해야 한다.

== 3/29 4.2 ==

IOCP를 쓰는 이유: 멀티스레드에서 효과가 좋음.

가져오기 GetQueuedCompletionStatus <-> 추가하기 PostQueuedCompletionStatus 
근데 PQCS는 I/O에서 사용하지 않음. 엉망이 된다.

10p
device list는 커널에 저장되며 종료할떄까지 빼낼 수 없다.

사용하는 api
createIOCP
getQCS

- 강의자료 다름

1ㄷ1 방식에서는??
단계:
초기화
bind listen accept 호출
클라이언트 소켓을 iocp에 등록 후 recv 호출
메인 루프:
23번째줄

accept -> 1ㄷ1통신
accept를 비동기 호출이 필요 -> acceptEx 함수

- > 

1ㄷn 방식에서는????
초기화
bind/listen 소켓 iocp 등록
비동기 accpet 호출 (AcceptEx)
메인루프
123 모두

클라이언트 객체 (SESSION 객체) -> C_key으로 관리. 포인터가 필요가 없음.

IOCP는 이 I/O가 send인지 recv인지 알아낼 필요가 있음.

패킷을 사용하는 이유? 순서가 바뀌어도 실행하는 데 큰 무리가 없도록 하기 위함.
패킷이 섞이면 어떻게 하는가? 윈도우가 절대로 패킷을 섞지 않음.


100바이트 보낼때까지 기다리면 되지 않느냐?
운영체제 내부 메모리가 꽉 찼을 경우 
new가 불가능. 죽음. -> close 소켓해도 불가능.
-> 이런일이 발생하면 안댐 -> 자주 체크하고 send하지 말고 log out 시켜야함
-> 클라이언트마다 최대 보낼 크기를 정해둔다. -> 실습에는 구현x
 
=== 4/5 5.2 ==

TCB 안에는 context가 들어가 있다.
sp, pc

STACK0이 Stack1의 영역에 침범하게 되면 충돌할 수 있음.
STack을 어디까지 쓸 것인가? 를 고려하여 사용해야 한다.
-> 32바이트는 그 크기를 고려해야 했으나
64바이트를 사용하기 때문에 고려할 필요가 없음.

전역변수: 멀티스레드 시 관리가 힘듬.
어느 스레드가 건드리고 있는지 알기가 어려움.
-> 멀티스레드 환경에서 에러가 발생가능

멀티 프로세스 -> 멀티스레드
싱글 스레드보다 좋은 것
성능 향상
빠른 응답 속도
-> 한시간 계산해야 하는 패킷이다 -> 1시간동안 기달려야 함
-> 멀티스레드에서는 문제가 없음. 라운드 로빈.
더 나은 자원 활용
멀티 프로세스보다 좋은 것
통신
context switch

멀티스레드는 위험하다. 그러나 동접을 늘리기 위해서 사용해야 한다.

서버를 여러 개 키면 되지 않느냐??? ->
서버가 다르면 플레이어끼리 볼 수가 없음.

server는 homogenous
client는 Heterogenous

구현 난이도는 Heterogenous가 더 쉬움.
그러나 성능 향상에 있어선 Homogenous가 더 유리

너무  많은  쓰레드
-> 응답 속도가 느려짐.

thread 생성 시 초기자
1. 함수 2. 람다 3. 실행객체

==== 4/9 6.1 ====

왜 5천만번 돌렸는데 0ms가 걸리는가?
컴파일러가 루프를 돌지 않게끔 코드를 작성 -> volitile
+ CPU의 클럭 속도


멀티스레드 사용했을 때 오히려 속도가 느려짐. -> lock의 부하.
오히려 context switch때문에 thread가 많아질수록 느려짐.
-> 적절한 Lock의 크기를 설정하는게 필요하다!!!

멀스가 어려운 이유.
성능을 끌어올리기가 어렵다.
->  프로그램을 새로 짜야 한다.


==== 4/12 6.2 ====

7-8 사이
c++20에서 사용하는 atomic<shared_ptr<T>>을 사용
nonblocking이 아님 -> 많이 느림

성능 저하의 원인: mutex와 reference counterㅌ
전역변수에는 atomic을 사용하지 말자.

map에 Session을 저장하는 데에는 필수.
boost::aiso에서는 필수다.

성능 저하를 감수할 만 한가?

멤버변수를 하나하나 따져서 data race가 발생하는 멤버만 따로 락을 건다.

== 7.2 ==

시야처리

서버의 부하가 너무 크다.

Zone로 쪼개도 사람이 몰리는 장소가 있다
-> Zone을 Sector 분할 -> 검색 오버헤드를 줄이자. -> 자신과 인접 sector만 검색.


=====

path mesh == navigation mesh라고도 불림.

몬스터가 길찾기에 실패했을 경우?
길찾기가 실패하지 못하게 한다.
몬스터에 막혔을 때면 잠깐 기다렸다가 길찾기를 다시 한다.
힐시켜 어그로가 풀리게 한다.


IDDF
조금씩 거리를 늘리면서 목표가 가능한 길을 찾음 -> 사용하지 않음.


Breadth-first

다익스트라를 게임 입장에서 최적화 한 것이 A*
그냥쓰면 n2. 최적화 필요. -> bidrectional. 목적지와 시작지를 모두 계산

best-first search -> 최단거리를 찾지 못함

A*: 다익스트라는 무조건 최단 거리를 찾는데 방향이 있기 때문에
절대 거리가 가까운 쪽으로 우선탐색
그냥 돌리면 오버헤드가 크므로 시간을 둬서 시간을 넘기면 fail

AI thread를 만드는 것이 더 직관적.


실제 게임서버는 상속을 주로 많이 사용한다.


